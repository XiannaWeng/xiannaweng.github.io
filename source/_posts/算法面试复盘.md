---
title: 八股速记
date: 2025-05-17 19:00:00
tags:
  - 八股
  - 面试
categories:
  - 八股
---
# 八股速记

## 1 大模型基础

### 1.1 分词

+ **分词：**本质就是要覆盖所有语料的情况下，分得尽可能少，分得有语义
+ **分词的粒度：**词粒度、字符粒度、子词粒度
+ **Byte Pair Encoding(BPE)**：首先基于基础词表统计词出现的次数，然后再将两个连续的词或者字符，从高频到低频进行合并。词表大小是先增加后减少的。
+ **Byte-level BPE(BBPE)：**由于字符级别的分词，在日语或者汉字中的稀有词太多，导致词表很大，所以用字节来进行编码，减少词表，防止OOV。
+ **WordPiece：**BPE和BBPE经常会有无语义的分词，WordPiece根据互信息对连续子词进行合并。score = p(z)/p(x)p(y)，让被合并的子词尽可能低频，同时合并后的高频。
+ **Unigram Language Model(ULM)：**ULM的思路是“分裂”，初始化大单元，然后根据大单元不同拆分方式中，子词的最大似然度（子词频率乘积）计算loss，倾向于保留那些以较高频率出现在很多分词结果中的子词。
+ **常用分词库：**SentencePiece（支持无空格语言）, Tokenizers（Hugging Face，预分词）

### 1.2 Embedding 词嵌入

本质就是将自然语言的词，单射且同构地将高维度的数据转换为低纬度的向量，以便于计算机理解。具体来说，就是随机化初始向量形成可学习参数矩阵，以onehot为输入，向量为输出。

+ **OneHot：**只有0 1数值，本质上是一种编码，极度稀疏，会造成词的维度爆炸。
+ **Word2Vec：**将词映射到向量上，有两种思路
  + **CBOW**：上下文 → 目标词（多对一），训得快，但语义捕捉弱。
  + **Skip-gram**：目标词 → 上下文（一对多），训得慢，但语义捕捉强。
+ **Word2Vec加速**方法：
  + **霍夫曼树：**避免计算所有词的softmax概率，根据词频来建立树，将多分类问题转为二分类。计算量从V变成了logV。
  + **负采样：**每次取一个正样本，随机取五个副样本进行权重更新。
+ **FastText：**快速文本分类，和CBOW一样，不过输入是多个单词对和对应文档的n-gram特征，输出是文档类别，也是用了霍夫曼树的层次softmax。

### 1.3 注意力

+ **Softmax**：多个值映射到0-1区间内

  ![image-20250517082554322](https://fantasticnana.xyz/2025/05/c7310b26a8075596fa8cbe59db206dd1.png)

+ **Sigmoid**：将一个值映射到0-1区间内

  ![image-20250517082641489](https://fantasticnana.xyz/2025/05/4bfed361acb35cf8d8c1d15a7e4f25ef.png)

+ **自注意力**：在预测下一个词时，动态分配过去词的注意力权重

  ![image-20250517082946392](https://fantasticnana.xyz/2025/05/d362a30da0705d388f4ba9f06997016d.png)

+ **交叉注意力**：QKV不是来自一个序列，而是来自不同序列，在机器翻译上比较好使。

+ **多头注意力**：将词的嵌入向量沿着特征维度拆成几个，分别QKV，这样有助于学习到词在不同维度的语义。

  **特征维度**（hidden size）：就是embedding向量的长度。

+ **Spare attention 稀疏注意力**：除了相对距离不超过k的，k 2k 3k都设置为0，局部紧密相关和远程稀疏相关，减少计算量。

+ **线性注意力**：将softmax拿掉，算法复杂度从平方转换成线性。

  + **公式**：这里分母底下的全1向量用于求和QK，从而进行归一化。

    ![image-20250517090003503](https://fantasticnana.xyz/2025/05/538c16fc2ad5184efac67cff8d43d161.png)

  + **核函数**：将向量矩阵从高维到低维，或者低维到高维。这里常见的有：

![image-20250517085929990](https://fantasticnana.xyz/2025/05/797ab1cff84f1f0f6829dc789e209b2e.png)

+ **KV Cache**：causal attention的下一个词预测计算需要用到前面的KV，使用缓存加速。
  + **causal attention**，因果注意力，也就是当前主流的Musk掩码，在预测下一个词时，只知道前面的词。

+ **Dual Chunk Attention**：长序列固定拆块儿，块内计算QKV，块儿之间再算一遍，可以有效处理长文本。

### 1.4 FNN & ADD & LN

+ **FNN 前馈神经网络：**提取QKV得到的矩阵的信息，一般由俩全连接层和一个非线性激活层组成。

  ![image-20250517093508895](https://fantasticnana.xyz/2025/05/7abf3404e2323fdd69f8ad926dbc5adb.png)

+ **ADD 残差连接**：将子层的输入和输出相加，缓解梯度消失。

+ **LN 层归一化**：将特征分布拉到0附近，标准化。

  ![image-20250517094034492](https://fantasticnana.xyz/2025/05/298a3149f35b4a92836aa35968c8d750.png)

### 1.5 位置编码

+ **绝对位置编码：**transformer的经典正余弦位置编码，将每个位置的token固定一个编码，加到embeding矩阵中去。

![image-20250517101026810](https://fantasticnana.xyz/2025/05/86d10ffa87798de3538dde9ee6cea0a7.png)

+ **相对位置编码**：例如旋转位置编码RoPE，对Q K矩阵注入了旋转信息。

  ![image-20250517102635222](https://fantasticnana.xyz/2025/05/a2b10f5570fa2e2269fbe65b80d57e8e.png)

  ![image-20250517102124183](https://fantasticnana.xyz/2025/05/81dd10d7f0a31e5d67400f3e161ca9e5.png)

### 1.6 结构和解码

+ Encoder：MHA+FFN+ADD&LN，双向注意力，预测下一个词不做Musk。
+ Decoder：Masked MHA+LN&ADD，单向注意力，预测下一个词只能用之前的信息。
+ Dense Model 稠密模型：每次推理会激活全部参数。
+ 场景选择：
  	1. 文本分类、实体识别：Encoder-only 双向注意力
  	1. 文本生成：Decoder-Only 单向注意力。
  	1. 序列到序列任务，如典型的翻译场景：输入双向、输出单向。

+ MOE 混合专家：门控网络来决定激活那些专家网络，也就是FNN层。
+ 解码策略：
  1. 选取概率最高的Token：会使得输出文本单调且重复。
  2. Top-K Sampling：从排名前K的token中进行抽样。

## 2 预训练

### 2.1 数据处理

+ 获取URL
+ 使用正则表达式进行URL过滤
+ 内容抽取
+ 语言识别，使用FastText过滤非语言部分
+ 低质过滤，正则、困惑度等
+ 模型打分，用Bert-base/FastText进行微调评分
+ 数据去重，分unit，先unit内再unit之间去重
+ 筛选垂域数据，关键词初筛、相似度召回、人工筛选、用这部分高质量数据训分类器、筛更多数据

### 2.2 预训练流程

+ 训练tokeniser
+ 词表扩充
+ 确定模型结构和参数
+ 训练框架选择、训练参数调整
+ 监控训练
+ 继续预训练

### 2.3 预训练评估

+ PPL困惑度：主要评估模型在预测下一个词的概率是否分散

  ![image-20250517150529083](https://fantasticnana.xyz/2025/05/436c23643e4167cf7e91ccecfb1213ae.png)

+ BenchMark
+ 大海捞针
+ 概率探针

## 3 后训练

### 3.1 SFT 监督微调

本质就是让掌握了通用知识的大模型，可以通过指令遵循的方式，执行工作。

+ 几个SFT共识：

  prompt的质量和多样性极端重要，数量需求比较少。

  可以加点预训练数据进去，减轻灾难性遗忘

  不能做太多知识注入，不能有太多超过模型本身能力的问答对

+ 合成数据：
  + 通过GPT4进行合成问答对数据
  + 拒绝采样，一个问题多个推理路径，Chosen 和 Rejected偏好数据，然后进行SFT或者DPO
+ 数据质量过滤：
  + 用OpenAssistant的reward model打分
  + 用K - Center - Greedy算法进行过滤，最大化多样性最小化数据集
+ 多样性包括prompt的表达方式、难度、长度，answer的长度、多样性，多轮聊天的切换topic能力
+ SFT训练：一般采用OpenRLHF框架，常用参数如下：

| 参数名                          | 说明                                                    | 设置建议                                                     |
| ------------------------------- | ------------------------------------------------------- | ------------------------------------------------------------ |
| **epoch**                       | 训练数据遍历模型的轮数                                  | 通常设为 1；微调垂域模型且数据量 ≤1 万条时，可设为 3         |
| **gradient_accumulation_steps** | 梯度累积步数，全局批量大小 = 该值 × 单设备批量 × 设备数 | 根据实际计算资源与批量需求调整                               |
| **global_batch_size**           | 所有设备上的总批量大小（Megatron 参数）                 | 由单设备批量、设备数、gradient_accumulation_steps 计算，DeepSpeed 有类似设置（如 micro_train_batch_size 与 train_batch_size） |
| **learning_rate**               | 控制模型参数更新步长                                    | SFT 阶段通常为预训练阶段的 10 倍（如预训练 \(1.5e-6\)，SFT 用 \(3e-5\)） |
| **lr_scheduler_type**           | 学习率随训练变化的策略                                  | 常用 cosine（余弦衰减）                                      |
| **dropout**                     | 随机丢弃神经元防止过拟合的正则化技术                    | 一般不启用（效果有限且拖累训练效率）                         |
| zero_stage                      | DeepSpeed 优化阶段                                      | 显卡充足选 zero2；zero3 因通信成本高，训练慢，少用           |
| max_seq_len                     | 模型可处理的输入序列最大长度                            | 一般设为 4K（4096）                                          |
| offload                         | 将部分计算 / 数据从 GPU 转移到 CPU/NVMe                 | 一般不设置（避免数据迁移耗时）                               |
| gradient_checkpointing          | 通过重算中间结果降低内存占用                            | 启用后内存开销从 \(O(N)\) 降至 \(O(\sqrt{N})\)（N 为模型层数） |
| seq_parallel_size               | 分布式训练时输入序列的分块大小                          | 根据分布式配置调整，提升并行效率                             |
| weight_decay                    | 权重衰减正则化参数                                      | 默认 0.01，防止过拟合                                        |
| per_device_train_batch_size     | 每个 GPU 上的训练批量大小                               | 一般设为 1，可依硬件调整                                     |
| num_warmup_steps                | 训练初期热身步数                                        | 设为总训练步数的 5% - 10%（如总步数 10,000 时，选 500 - 1,000 步） |

+ SFT评估：Model来评、人评

### 3.2 RL强化学习

强化学习本质是找到一个动作执行策略，在给定的环境下拿到最大的奖励。

+ **马尔可夫决策**：前一个状态影响后一个状态

  ![image-20250517162902043](https://fantasticnana.xyz/2025/05/5fec6dee13d56694e78315ab3f5e3af4.png)

![image-20250517162924412](https://fantasticnana.xyz/2025/05/575a1e4f85b3c3d19db8e8ab57ea2b89.png)

+ **贝尔曼最优方程：**描述选择的动作最优策略，本质是递归

![image-20250517164051548](https://fantasticnana.xyz/2025/05/435c975ee16f57b55c0d7b41d3c142c8.png)

+ **蒙特卡洛方法：**其实就是大数定律，样本量足够多时能代表真实概率，在大模型RL这里就是对一个策略走多个路径，来近似未来奖励期望。

+ **动态规划方法**：适用于及其清晰的环境，其中MDP和环境动态完全已知，最优策略直接DP就可以了。

+ **时序差分方法：**适用于环境不能已知，需要走一步看一步的场景。而且也比蒙特卡洛高效，因为不用等路径走完。核心思想是用未来动作选择的价值估计，更新当前动作选择的价值估计。

![image-20250517172525629](https://fantasticnana.xyz/2025/05/9ced67c0b78e987b0be4d745028362a5.png)

+ **Q-Learning方法**：选下一个价值最高的动作+即时奖励，作为当前状态最优动作的估计，属于时序差分的一种应用。

  ![image-20250517172939951](https://fantasticnana.xyz/2025/05/0545c729087b27f2f37bef37e95fa385.png)

+ **RL种类划分：**

  + **Online/Offline**：一边与环境交互形成轨迹，一边学习策略；轨迹已经定好，不涉及环境交互；
  + **Model-base/Model-free**：环境转移方程、奖励函数模型构建好了，也不需要真正与环境交互；没有转移方程和奖励模型，需要直接和环境交互；
  + **On-Policy/Off-Policy**：行为策略和目标策略一致；不一致；
    + **行为策略**：实际用于选择动作的策略，On-Policy每一次都会根据当前行为策略选择之后的路径。
    + **目标策略**：利用行为策略生成的数据进行学习和更新的策略，Off-Policy中，比如Q-learning每一次都用贪婪策略模拟之后的路径。

  + **Value-based/Policy-based**：先学习值函数再导出策略；直接学习策略。













